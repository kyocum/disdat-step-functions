{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac3bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stepfunctions.steps import states\n",
    "from stepfunctions.workflow import Workflow\n",
    "from disdat_step_function.caching_wrapper import Caching, PipelineCaching, LambdaGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edbffe",
   "metadata": {},
   "source": [
    "## Create lambda code and layer \n",
    "Our code must run on something, the cheapest way to host disdat-step-functioin is AWS lambda. Therefore, please use the following code to generate the lambda function.\n",
    "\n",
    "**Warning**  \n",
    "The following command takes some time as it needs to build docker image + compressing liibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8edc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LambdaGenerator.generate(root='generated_code', force_rerun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d68f4c",
   "metadata": {},
   "source": [
    "What happened behind the scene was that we started a docker container with volume mount and run `pip install disdat` inside. By doing so, the install c binaries are compatible with AWS lambda runtime \n",
    "\n",
    "### Create Lambda on AWS\n",
    "Now we have the lambda code and layer zip, it is time to create a lambda function on AWS. There are a few things to bear in mind:\n",
    "1. The generated code is in /generated_code folder\n",
    "\n",
    "2. Make sure the lambda function has read and write permission to the S3 bucket you are using for data storage\n",
    "3. Make sure the lambda function use runtime python 3.8\n",
    "4. Don't forget to upload the layer zip file, as disdat is not available on AWS by default!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3cb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc344add",
   "metadata": {},
   "source": [
    "## Option 1 - state-level caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90dbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the caching object \n",
    "caching = Caching(caching_lambda_name=\"YOUR_LAMBDA_NAME\", # for instance 'cache_lambda_worker'\n",
    "                  s3_bucket_url='s3://YOUR_BUCKET',\n",
    "                  context_name='tutorial_context',\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d7e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = states.Pass(state_id='state_1')\n",
    "state_2 = states.Pass(state_id='state_2')\n",
    "state_3 = states.Pass(state_id='state_3')\n",
    "\n",
    "state_1 = caching.cache_step(state_1)\n",
    "state_2 = caching.cache_step(state_2)\n",
    "state_3 = caching.cache_step(state_3)\n",
    "\n",
    "definition = states.Chain([state_1, state_2, state_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a8f52",
   "metadata": {},
   "source": [
    "### Execute\n",
    "Go to the submission and execute section. You can check out the modified state machine on AWS console\n",
    "![state-level](docs/state_level.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022404d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598341b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176ffb6d",
   "metadata": {},
   "source": [
    "## Option 2 - pipeline-level caching \n",
    "If you have a long state machine with many states, it would be nice to refactor the pipeline with one line of code. \n",
    "Good news! Disdat-step-functions does have this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423da6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same state machine \n",
    "state_1 = states.Pass(state_id='state_1')\n",
    "state_2 = states.Pass(state_id='state_2')\n",
    "state_3 = states.Pass(state_id='state_3')\n",
    "# normal definition \n",
    "definition = states.Chain([state_1, state_2, state_3])\n",
    "\n",
    "\n",
    "caching = Caching(caching_lambda_name=\"YOUR_LAMBDA_NAME\", # for instance 'cache_lambda_worker'\n",
    "                  s3_bucket_url='s3://YOUR_BUCKET',\n",
    "                  context_name='tutorial_context',\n",
    "                  verbose=True)\n",
    "# cache the pipeline annnd done!\n",
    "PipelineCaching(definition, caching).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc41948",
   "metadata": {},
   "source": [
    "### Execute\n",
    "Go to the submission and execute section. You can check out the modified state machine on AWS console. This is what I obtained\n",
    "![pipeline](docs/pipeline_level.png)\n",
    "\n",
    "WAIT a second, why isn't the pipeline cached? This is because, by design,  `PipelineCaching(definition, caching).cache()` only replaces `states.Task`(where real work actually happens). Replacing states such as `Wait`, `Pass` doesn't make any sense + introduces a lot of overhead! \n",
    "\n",
    "If you replace `Pass` with proper `Task` objects, `PipelineCaching` will kick in and replace them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f643fde",
   "metadata": {},
   "source": [
    "### Pipeline-level caching fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9e27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same state machine \n",
    "state_1 = states.Task(state_id='state_1')\n",
    "state_2 = states.Task(state_id='state_2')\n",
    "state_3 = states.Task(state_id='state_3')\n",
    "\n",
    "# normal definition \n",
    "definition = states.Chain([state_1, state_2, state_3])\n",
    "\n",
    "caching = Caching(caching_lambda_name=\"YOUR_LAMBDA_NAME\", # for instance 'cache_lambda_worker'\n",
    "                  s3_bucket_url='s3://YOUR_BUCKET',\n",
    "                  context_name='tutorial_context',\n",
    "                  verbose=True)\n",
    "# cache the pipeline annnd done!\n",
    "PipelineCaching(definition, caching).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f2869",
   "metadata": {},
   "source": [
    "### Submit the workflow but DON'T execute it!\n",
    "`states.Task` needs more configuration, such as Lambda name or EC2 ARN, which obviously we don't have. If you execute the state machine, you'll get an error.   \n",
    "\n",
    "However, you should be able to see the augmented graph on AWS. Here's what I obtained:\n",
    "![task](docs/task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614d5de",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Executing the following commands requires a valid AWS credential. Please make sure your IAM role has access to StepFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08da854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_name = 'simple_cached_workflow'\n",
    "\n",
    "target_flow = [flow for flow in Workflow.list_workflows() if flow['name'] == workflow_name]\n",
    "\n",
    "# if the same workflow name is used, update the definition \n",
    "if len(target_flow) > 0:\n",
    "    workflow = Workflow.attach(target_flow[0]['stateMachineArn'])\n",
    "    workflow.update(definition=definition, role=config.EXECUTION_ROLE)\n",
    "# otherwise create a new one \n",
    "else:\n",
    "    workflow = Workflow(workflow_name, definition=definition, role=config.EXECUTION_ROLE)\n",
    "    workflow.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9903fa2",
   "metadata": {},
   "source": [
    "### Execute\n",
    "Supply some inputs to the state machine you just created and see what will happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5debdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'Hello': 'world'}\n",
    "execution = workflow.execute(inputs=inputs)\n",
    "result = execution.get_output(wait=True)\n",
    "# the state machine doesn't do anything, it just passes the data along \n",
    "assert result == inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec710a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
